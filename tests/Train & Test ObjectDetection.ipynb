{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fdc8fcb-bfb6-4fc9-9808-276d762f2c54",
   "metadata": {},
   "source": [
    "## Train & Test for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b9e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "%pip install albumentations pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83338f8c-ca6b-4e42-be71-79921cf83bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f'Torch version: {torch.__version__}\\n')\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    print(f'CUDA is available')\n",
    "    cuda_device = torch.cuda.current_device()\n",
    "    print(torch.cuda.get_device_name(cuda_device))\n",
    "    print(f'Compute capability: {torch.cuda.get_device_capability(cuda_device)}')\n",
    "else:\n",
    "    print('CUDA is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e6822-3cdf-4fc4-9f1b-93ae37c2abe9",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "COCO_PATH = join(os.getcwd(), 'coco')\n",
    "ANNOTATIONS_PATH = join(COCO_PATH, 'result.json')\n",
    "\n",
    "\n",
    "# mask = Image.open('mask.png').convert('L')\n",
    "# mask = np.array(mask)\n",
    "\n",
    "\n",
    "def transformations(img, target):\n",
    " \n",
    "    img = np.array(img)\n",
    "    bboxes = [obj['bbox'] for obj in target]\n",
    "    \n",
    "    transform = A.Compose([\n",
    "        # A.Crop(y_min=240, y_max=img.shape[0], x_min=0, x_max=img.shape[1]),\n",
    "        A.Resize(256, 256),\n",
    "        A.RandomBrightnessContrast(p=0.3),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ], bbox_params=A.BboxParams(format='coco', min_visibility=0.6, label_fields=['labels']))\n",
    "    \n",
    "    transformed = transform(image=img, bboxes=bboxes, labels=[obj['category_id'] for obj in target])\n",
    "    \n",
    "    target = {\n",
    "        'boxes': torch.tensor(transformed['bboxes'], dtype=torch.float32),\n",
    "        'labels': torch.tensor(transformed['labels'], dtype=torch.int64),\n",
    "    }\n",
    "\n",
    "    return transformed['image'], target\n",
    "\n",
    "\n",
    "coco_dataset = datasets.CocoDetection(root=COCO_PATH, annFile=ANNOTATIONS_PATH, transforms=transformations)\n",
    "\n",
    "CATEGORIES = {category_id: category_info['name'] for category_id, category_info in coco_dataset.coco.cats.items()}\n",
    "print(CATEGORIES)\n",
    "\n",
    "coco_default_dataset = datasets.CocoDetection(root=COCO_PATH, annFile=ANNOTATIONS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ebc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, annotations1 = coco_dataset[0]\n",
    "image2, annotations2 = coco_default_dataset[0]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import ImageDraw\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "def drawBoundingBoxes(image: Image.Image, boxes: torch.Tensor, labels: torch.Tensor):\n",
    "    output = np.array(image)\n",
    "    \n",
    "    if output.shape[0] == 3:\n",
    "        output = np.transpose(output, (1, 2, 0))\n",
    "    \n",
    "    for (x1, y1, x2, y2), label in zip(boxes, labels):\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x1)+int(x2), int(y1)+int(y2)\n",
    "        cv2.rectangle(output, (x1, y1), (x2, y2), (255, 0, 0), 1)\n",
    "        cv2.putText(output, CATEGORIES[int(label)], (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(drawBoundingBoxes(image1, annotations1['boxes'], annotations1['labels']))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Transformed')\n",
    "\n",
    "\n",
    "axes[1].imshow(drawBoundingBoxes(image2, torch.tensor([obj['bbox'] for obj in annotations2]), torch.tensor([obj['category_id'] for obj in annotations2])))\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Default')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28617a13",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "- Resize the image to the desired size.\n",
    "- Normalize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb54e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "COCO_PATH = join(os.getcwd(), 'coco')\n",
    "ANNOTATIONS_PATH = join(COCO_PATH, 'result.json')\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, annotations, transform=None) -> None:\n",
    "        self.root = root\n",
    "        self.categories = self.loadCategories(annotations)\n",
    "        self.annotations = self.loadAnnotations(annotations)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def loadCategories(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        parsed_categories = {}\n",
    "        for category in data['categories']:\n",
    "            parsed_categories[category['id']] = category['name']\n",
    "        return parsed_categories\n",
    "    \n",
    "    def loadAnnotations(self, path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        parsed_annotations = []\n",
    "        for annotation in data['annotations']:\n",
    "            image_id = annotation['image_id']\n",
    "            category_id = annotation['category_id']\n",
    "            image_path = data['images'][image_id]['file_name']\n",
    "            bbox = annotation['bbox']\n",
    "\n",
    "            parsed_annotations.append((image_id, category_id, image_path, bbox))\n",
    "\n",
    "        return parsed_annotations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id, category_id, image_path, bbox = self.annotations[index]\n",
    "        image = Image.open(join(self.root, image_path)).convert('RGB')\n",
    "        bbox = [bbox['x_min'], bbox['y_min'], bbox['width'], bbox['height']]\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, bboxes=[bbox])\n",
    "            image = transformed['image']\n",
    "            bbox = transformed['bboxes'][0]\n",
    "\n",
    "        return image, bbox, category_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "# coco_dataset = datasets.CocoDetection(root=COCO_PATH, annFile=ANNOTATIONS_PATH, transform=transform)\n",
    "\n",
    "dataset = CustomDataset(root=COCO_PATH, annotations=ANNOTATIONS_PATH)\n",
    "print(len(dataset))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
